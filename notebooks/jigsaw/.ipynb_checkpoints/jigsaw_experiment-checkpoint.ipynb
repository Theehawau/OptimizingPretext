{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adbb0764-f13b-4afd-9bd7-5fdd58b63f8f",
   "metadata": {},
   "source": [
    "# JigSaw pretext task\n",
    "Following [the original implementation](https://arxiv.org/pdf/1603.09246) <br>\n",
    "Also useful to look at [the FAIR paper](https://arxiv.org/pdf/1905.01235) (page 12), for details on the implementation.<br>\n",
    "Adapted to use ResNet18 instead of CFN.\n",
    "TODO\n",
    "- Organize into separate .py modules for JigSaw utils\n",
    "- Create runner script that can use up to 4 GPUs for faster training.\n",
    "- Add ViT\n",
    "- Change training dataset to something w/ resolution of ~255x255 to avoid the need to upscate data\n",
    "- Add the evaluations -> basically take the (pretrained) resnet module and plug it into another module w/ a clean classification head \n",
    "    - linear probing\n",
    "    - full ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "150c7425-5ca5-41b9-b507-1e58b5eef889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05cdc376-2f79-4e82-85df-9400a180bb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f13f6ce-5c49-4582-a48f-5cf9007a7518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x145da6bb3190>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set random seed for reproducibility -> maybe use pytorch lightning for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798b2f0d-4b60-42b4-ab97-310fa9992e09",
   "metadata": {},
   "source": [
    "## Load Tiny-ImageNet/ ImageNet-1k from HF\n",
    "Should download imagenet1k and evaluate there, but first update the path to download datasets to https://discuss.huggingface.co/t/specifying-download-directory-for-custom-dataset-loading-script/11150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28338577-abf1-46aa-9c07-c5f4766df2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_path = '/l/users/emilio.villa/huggingface/datasets'\n",
    "datasets.config.DOWNLOADED_DATASETS_PATH = Path(download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f139d43-ccdb-4e7e-b145-55e7a2aaf561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"zh-plus/tiny-imagenet\", cache_dir = download_path)\n",
    "## We can also download it from here http://cs231n.stanford.edu/tiny-imagenet-200.zip but i think HFs its easier\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e44e87f-435d-4b69-8372-9d8a395a230e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7feb847117334c9aa55f59ba4f60ad7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/257 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac160eb150bf4a929a29b80738a7a7b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 1281167\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 100000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## imagenet1k\n",
    "access_token = \"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\"\n",
    "dataset = load_dataset('ILSVRC/imagenet-1k', token = access_token, cache_dir = download_path)#, num_proc=8)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f3f690-4089-4f6b-876f-7efa5644439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_1k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed96c94-df0f-429f-9f66-139dd1e24f72",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f475fa7d-6a08-4ece-9063-5a78f5cf4220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_permutations(n_permutations, n_tiles):\n",
    "    \"\"\"\n",
    "    Generates a list of permutations, these will essentially be the 'gold truth' labels\n",
    "    \"\"\"\n",
    "    permutations = []\n",
    "    seen = set()\n",
    "    while len(permutations) < n_permutations:\n",
    "        perm = tuple(np.random.permutation(n_tiles))\n",
    "        if perm not in seen:\n",
    "            permutations.append(perm)\n",
    "            seen.add(perm)\n",
    "    return permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07d220da-4f5d-49fa-8032-73a5a69bdc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate n_permutations permutations for 9 tiles, these values are related to the complexity of the task and should be updated as necessary\n",
    "n_permutations = 1000 ##essentially the number of classes\n",
    "n_tiles = 9\n",
    "permutations = generate_permutations(n_permutations, n_tiles)\n",
    "\n",
    "# Enumerate and store permutations\n",
    "permutations_dict = {i: perm for i, perm in enumerate(permutations)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfc9dcc6-d17b-49a3-98e0-1337e398ea4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These indices will basically be the possible \"gold\" labels :p\n"
     ]
    }
   ],
   "source": [
    "permutations[:3] \n",
    "print('These indices will basically be the possible \"gold\" labels :p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f46e30a4-b111-4a11-a2a8-b3eaeb7cf785",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JigsawPuzzleDataset(data.Dataset):\n",
    "    def __init__(self, hf_dataset, permutations, transform=None):\n",
    "        \"\"\"\n",
    "        Paramteres:\n",
    "            hf_dataset: HuggingFace Dataset object.\n",
    "            permutations: List of permutations.\n",
    "            transform: Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.dataset = hf_dataset\n",
    "        self.permutations = permutations\n",
    "        self.n_permutations = len(permutations)\n",
    "        self.n_tiles = 9  # 3x3 grid -> this can be modified later\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image from the HuggingFace dataset and convert to RGB\n",
    "        image = self.dataset[idx]['image'].convert('RGB')  # Ensure image is in RGB\n",
    "\n",
    "        # resize it to 255x255 we will reduce to 64x64 patches\n",
    "        image = image.resize((255, 255))\n",
    "\n",
    "        # divide the image into 3x3 grid of tiles (85x85 pixels each)\n",
    "        tiles = []\n",
    "        tile_size = 85 # 85 * 3 = 255\n",
    "\n",
    "        ## Iterate over possible tiles and create the patches\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "\n",
    "                '''\n",
    "                Original paper explanation: We randomly crop a 225 × 225 pixel window from an image (red dashed box), divide it into a 3 × 3 grid, and randomly pick a 64 × 64 pixel tiles from each 75 × 75 pixel cell.\n",
    "                '''\n",
    "\n",
    "                # Get boundaries and crop\n",
    "                left = j * tile_size\n",
    "                upper = i * tile_size\n",
    "                right = left + tile_size\n",
    "                lower = upper + tile_size\n",
    "                tile = image.crop((left, upper, right, lower))\n",
    "                \n",
    "                # Random crop of 64x64 pixels with random shifts\n",
    "                shift_max = tile_size - 64  # Max shift to introduce randomness\n",
    "                left_shift = random.randint(0, shift_max)\n",
    "                upper_shift = random.randint(0, shift_max)\n",
    "                tile = tile.crop((left_shift, upper_shift, left_shift + 64, upper_shift + 64))\n",
    "                \n",
    "                # Apply any transform passed as argument\n",
    "                if self.transform is not None:\n",
    "                    tile = self.transform(tile)\n",
    "                    \n",
    "                tiles.append(tile)\n",
    "                \n",
    "        # Select a random permutation from the pre-computed permutations\n",
    "        perm_idx = random.randint(0, self.n_permutations - 1)\n",
    "        perm = self.permutations[perm_idx]\n",
    "\n",
    "        # Shuffle the tiles according to the permutation\n",
    "        shuffled_tiles = [tiles[p] for p in perm]\n",
    "\n",
    "        # Stack tiles into a tensor\n",
    "        tiles_tensor = torch.stack(shuffled_tiles, dim=0)  # Shape: [9, 3, 64, 64]\n",
    "\n",
    "        # Return the shuffled tiles and the permutation index which is the gold label we aim the model to predict\n",
    "        return tiles_tensor, perm_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "faf96c46-4627-435f-903f-27e7a4a8152a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5812c32c-da8c-46af-af6a-b14f1fdb6ec6",
   "metadata": {},
   "source": [
    "## Instance the torch module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b3e6a08-0cf7-4bab-b757-a333b4dcbba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JigsawNet(nn.Module):\n",
    "    def __init__(self, \n",
    "                 n_permutations,\n",
    "                 architecture = 'resnet', # 'resnet' or 'vit'\n",
    "                ):\n",
    "        \n",
    "        super(JigsawNet, self).__init__()\n",
    "\n",
    "        if architecture=='resnet':\n",
    "            # Backbone ResNet model TODO: replace by ResNet 50\n",
    "            self.resnet = models.resnet18(weights=None) \n",
    "            \n",
    "            self.resnet.fc = nn.Identity()  #Remove the classification layer\n",
    "            \n",
    "        elif architecture=='vit':\n",
    "            pass ##TODO\n",
    "\n",
    "        # Fully connected layers << to dispose after the PTT\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512 * 9, 4096), # each genertaes a 512-dimensional vector\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, n_permutations)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, 9, 3, 64, 64]\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # combine batch and tile dimensions (siamese network -> feed the same weights all the patches at once)\n",
    "        x = x.view(batch_size * 9, 3, 64, 64)  \n",
    "        features = self.resnet(x)  # Shape: [batch_size * 9, 512]\n",
    "\n",
    "        # concatenate the patches before the linear layers that learns to predict the permutation\n",
    "        features = features.view(batch_size, 9 * 512)  # shape -> [batch_size, 9 * 512]\n",
    "\n",
    "        #\n",
    "        out = self.fc(features)  # shape: [batch_size, n_permutations]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db629540-6598-423a-9d30-fee32e1774a8",
   "metadata": {},
   "source": [
    "## Training\n",
    "Hyperparameters should be adapted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d64757dc-f103-423a-a390-7d48d9eb6539",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = JigsawNet(n_permutations=n_permutations)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9913e7fd-8f14-41b9-bf64-2aa8a915e326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the datasets and dataloaders\n",
    "train_dataset = JigsawPuzzleDataset(dataset['train'], permutations, transform=transform)\n",
    "# valid_dataset = JigsawPuzzleDataset(dataset['valid'], permutations, transform=transform)\n",
    "valid_dataset = JigsawPuzzleDataset(dataset['validation'], permutations, transform=transform)\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=0)\n",
    "valid_loader = data.DataLoader(valid_dataset, batch_size=256, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c908c9da-0688-4539-9573-4ca3f130ce13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset output: torch.Size([256, 9, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "print('Shape of dataset output: {}'.format(next(iter(train_loader))[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea7fb4f0-78c1-4111-9902-e64e585d1575",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fdef3072-51e9-43d9-8a2a-b4e2ddcf15b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n",
    "num_epochs = 30\n",
    "log_each = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c09a4366-d7c3-4d91-9a06-8cb873b09395",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'jigsaw_rn50_imnt1k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72def79d-4d44-47de-bd29-b4d110666c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 39/5005 [02:28<5:10:53,  3.76s/it]/home/emilio.villa/miniconda3/envs/nlp24/lib/python3.12/site-packages/PIL/TiffImagePlugin.py:714: UserWarning: Metadata Warning, tag 274 had too many entries: 4, expected 1\n",
      "  warnings.warn(\n",
      "  2%|▏         | 100/5005 [06:12<4:58:45,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Batch [100], Loss: 6.9366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 200/5005 [12:21<4:54:15,  3.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Batch [200], Loss: 6.9307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 300/5005 [18:42<4:49:00,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Batch [300], Loss: 6.9345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 400/5005 [24:52<4:39:24,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Batch [400], Loss: 6.9312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 500/5005 [30:58<4:42:15,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Batch [500], Loss: 6.9347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 600/5005 [37:01<4:28:26,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Batch [600], Loss: 6.9344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 659/5005 [40:37<4:31:35,  3.75s/it]/home/emilio.villa/miniconda3/envs/nlp24/lib/python3.12/site-packages/PIL/TiffImagePlugin.py:900: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      " 14%|█▍        | 700/5005 [43:08<4:25:41,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Batch [700], Loss: 6.9349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 800/5005 [49:14<4:13:45,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Batch [800], Loss: 6.9311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 900/5005 [57:41<6:47:06,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Batch [900], Loss: 6.9330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 1000/5005 [1:06:29<6:07:00,  5.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Batch [1000], Loss: 6.9339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 1067/5005 [1:11:59<5:32:52,  5.07s/it]/home/emilio.villa/miniconda3/envs/nlp24/lib/python3.12/site-packages/PIL/TiffImagePlugin.py:900: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n",
      " 22%|██▏       | 1100/5005 [1:14:43<5:47:08,  5.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Batch [1100], Loss: 6.9354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 1200/5005 [1:22:22<4:54:56,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Batch [1200], Loss: 6.9348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 1300/5005 [1:30:49<5:01:24,  4.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Batch [1300], Loss: 6.9332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 1400/5005 [1:38:54<4:40:56,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Batch [1400], Loss: 6.9354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 1500/5005 [1:46:46<4:55:10,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Batch [1500], Loss: 6.9328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 1600/5005 [1:55:04<4:46:43,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Batch [1600], Loss: 6.9340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 1700/5005 [2:03:50<5:18:17,  5.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Batch [1700], Loss: 6.9331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 1800/5005 [2:10:44<3:18:01,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Batch [1800], Loss: 6.9329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 1900/5005 [2:17:13<3:31:04,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Batch [1900], Loss: 6.9318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 2000/5005 [2:23:48<3:20:34,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Batch [2000], Loss: 6.9311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 2100/5005 [2:30:08<3:03:05,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Batch [2100], Loss: 6.9331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 2200/5005 [2:36:24<2:56:21,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Batch [2200], Loss: 6.9321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 2300/5005 [2:42:48<2:44:25,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Batch [2300], Loss: 6.9325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 2322/5005 [2:44:11<2:41:10,  3.60s/it]/home/emilio.villa/miniconda3/envs/nlp24/lib/python3.12/site-packages/PIL/TiffImagePlugin.py:900: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      " 48%|████▊     | 2400/5005 [2:49:19<3:04:24,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Batch [2400], Loss: 6.9306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 2500/5005 [2:55:57<2:35:04,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Batch [2500], Loss: 6.9317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 2658/5005 [3:06:28<2:44:39,  4.21s/it]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    avg_loss = 0.0\n",
    "    for batch_idx, (tiles, perm_idx) in enumerate(tqdm(train_loader)):\n",
    "        tiles = tiles.to(device)  # Shape: [batch_size, 9, 3, 64, 64]\n",
    "        perm_idx = perm_idx.to(device)  # Shape: [batch_size]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(tiles)  # Shape: [batch_size, n_permutations]\n",
    "        loss = criterion(outputs, perm_idx)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss += loss.item()\n",
    "        if batch_idx % log_each == log_each - 1:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}], Loss: {avg_loss / 100:.4f}')\n",
    "            avg_loss = 0.0\n",
    "            \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for tiles, perm_idx in tqdm(valid_loader):\n",
    "            tiles = tiles.to(device)\n",
    "            perm_idx = perm_idx.to(device)\n",
    "\n",
    "            outputs = model(tiles)\n",
    "            loss = criterion(outputs, perm_idx)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += perm_idx.size(0)\n",
    "            correct += (predicted == perm_idx).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * correct / total\n",
    "    avg_val_loss = val_loss / len(valid_loader)\n",
    "    print(f'Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "    if val_accuracy > best_accuracy:\n",
    "        print('Saving checkpoint')\n",
    "        torch.save(model.state_dict(), f'{model_name}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6508b1a2-9f38-4e5a-9533-42947f51bc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45d5bcae-1922-49de-bdf1-aad410978c0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## save only resnet\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mresnet\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_resnet.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_name' is not defined"
     ]
    }
   ],
   "source": [
    "## save only resnet\n",
    "torch.save(model.resnet.state_dict(), f'{model_name}_resnet.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5374b650-12e9-4503-b4cf-4859ea789e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:03<00:00,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.8383, Validation Accuracy: 68.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "val_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "## just evaluate\n",
    "with torch.no_grad():\n",
    "    for tiles, perm_idx in tqdm(valid_loader):\n",
    "        tiles = tiles.to(device)\n",
    "        perm_idx = perm_idx.to(device)\n",
    "\n",
    "        outputs = model(tiles)\n",
    "        loss = criterion(outputs, perm_idx)\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += perm_idx.size(0)\n",
    "        correct += (predicted == perm_idx).sum().item()\n",
    "\n",
    "val_accuracy = 100 * correct / total\n",
    "avg_val_loss = val_loss / len(valid_loader)\n",
    "print(f'Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372fee5c-33a8-4766-a48b-c2bf3bedc73c",
   "metadata": {},
   "source": [
    "### Training linear classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "192b32dd-6822-4370-92ba-75f8bd15a753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JigsawNet(\n",
       "  (resnet): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=4608, out_features=4096, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = JigsawNet(n_permutations=n_permutations).to(device)\n",
    "model_path = '/home/emilio.villa/nlp_local/cv_ptt/jigsaw_rn18_tinyimnt.pth'\n",
    "model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37a6c8cf-f8ae-468e-82b7-826c089dc3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save only resnet\n",
    "model_rn_path = '/home/emilio.villa/nlp_local/cv_ptt/jigsaw_rn18_tinyimnt_resnet.pth'\n",
    "torch.save(model.resnet.state_dict(), model_rn_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac993cd9-594a-42df-a289-a00cf2d3e5be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f909e9c5-2828-484a-9af7-c8bdbf071a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_classes,\n",
    "        architecture = 'resnet', #'resnet \n",
    "        ):\n",
    "        super(ClassificationModel, self).__init__()\n",
    "        # Use the pretrained ResNet model from the PTT task\n",
    "        \n",
    "        if architecture=='resnet':\n",
    "            # Backbone ResNet model TODO: replace by ResNet 50\n",
    "            self.features = models.resnet18() \n",
    "            # self.resnet.fc = nn.Identity()\n",
    "            # self.features.fc = nn.Identity()  #Remove the classification layer ### IS this necessary??\n",
    "            \n",
    "        elif architecture=='vit':\n",
    "            pass ##TODO\n",
    "            \n",
    "        # Classification layer\n",
    "        self.linear_proj = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, 3, H, W]\n",
    "        features = self.features(x)  # Shape: [batch_size, 512]\n",
    "        out = self.linear_proj(features)  # Shape: [batch_size, num_classes]\n",
    "        return out\n",
    "\n",
    "\n",
    "# Instantiate the transfer learning model\n",
    "num_classes = 200  # Tiny ImageNet has 200 classes\n",
    "# resnet_model = model.resnet\n",
    "classifier = ClassificationModel(num_classes=num_classes, architecture = 'resnet')\n",
    "classifier = classifier.to(device)\n",
    "\n",
    "# for param in classifier.features.parameters():\n",
    "#     param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b99f4bd9-647b-4063-973a-dd85a1f769ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load pretrained resnet weights into the CLF model\n",
    "# pretrained_path = '/home/emilio.villa/nlp_local/cv_ptt/jigsaw_rn18_tinyimnt_resnet.pth'\n",
    "# classifier.features.load_state_dict(torch.load(pretrained_path, weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "445138a9-1f01-49c8-967b-7f837b5af18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## freeze backbone parameters (linear_probing) or don't do this for the\n",
    "for param in classifier.features.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "42a07b96-3e07-44c5-b110-a56a47811de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transforms\n",
    "classification_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Define custom dataset class\n",
    "class ClassificationDataset(data.Dataset):\n",
    "    def __init__(self, hf_dataset, transform=None):\n",
    "        self.dataset = hf_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.dataset[idx]['image'].convert('RGB')\n",
    "        label = self.dataset[idx]['label']\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Instantiate datasets and data loaders\n",
    "train_classification_dataset = ClassificationDataset(tinyImageNet_dataset['train'], transform=classification_transform)\n",
    "val_classification_dataset = ClassificationDataset(tinyImageNet_dataset['valid'], transform=classification_transform)\n",
    "\n",
    "train_classification_loader = data.DataLoader(\n",
    "    train_classification_dataset, batch_size=64, shuffle=True, num_workers=0\n",
    ")\n",
    "val_classification_loader = data.DataLoader(\n",
    "    val_classification_dataset, batch_size=64, shuffle=False, num_workers=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a74caf59-19ac-42ab-bd48-85fb72c86554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [02:27<00:00, 10.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 5.1392, Train Accuracy: 2.68%\n",
      "Validation Loss: 4.9973, Validation Accuracy: 3.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [02:37<00:00,  9.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 4.9252, Train Accuracy: 4.47%\n",
      "Validation Loss: 4.8776, Validation Accuracy: 4.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [02:45<00:00,  9.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/15], Loss: 4.8237, Train Accuracy: 5.38%\n",
      "Validation Loss: 4.8147, Validation Accuracy: 5.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [02:44<00:00,  9.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/15], Loss: 4.7573, Train Accuracy: 6.05%\n",
      "Validation Loss: 4.7609, Validation Accuracy: 6.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [02:30<00:00, 10.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/15], Loss: 4.7052, Train Accuracy: 6.87%\n",
      "Validation Loss: 4.7215, Validation Accuracy: 6.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [02:49<00:00,  9.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/15], Loss: 4.6665, Train Accuracy: 7.15%\n",
      "Validation Loss: 4.6870, Validation Accuracy: 6.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [02:34<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/15], Loss: 4.6338, Train Accuracy: 7.60%\n",
      "Validation Loss: 4.6672, Validation Accuracy: 7.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [02:55<00:00,  8.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/15], Loss: 4.6013, Train Accuracy: 7.87%\n",
      "Validation Loss: 4.6382, Validation Accuracy: 7.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [02:26<00:00, 10.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/15], Loss: 4.5801, Train Accuracy: 8.12%\n",
      "Validation Loss: 4.6449, Validation Accuracy: 7.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [02:49<00:00,  9.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/15], Loss: 4.5573, Train Accuracy: 8.68%\n",
      "Validation Loss: 4.6266, Validation Accuracy: 7.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [02:27<00:00, 10.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/15], Loss: 4.5364, Train Accuracy: 8.79%\n",
      "Validation Loss: 4.6232, Validation Accuracy: 7.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [02:33<00:00, 10.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/15], Loss: 4.5180, Train Accuracy: 9.10%\n",
      "Validation Loss: 4.6077, Validation Accuracy: 7.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [02:26<00:00, 10.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/15], Loss: 4.5026, Train Accuracy: 9.20%\n",
      "Validation Loss: 4.5907, Validation Accuracy: 8.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 1217/1563 [01:54<00:32, 10.62it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     11\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(train_classification_loader):\n\u001b[1;32m     13\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp24/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp24/lib/python3.12/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp24/lib/python3.12/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp24/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[40], line 20\u001b[0m, in \u001b[0;36mClassificationDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     18\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m---> 20\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp24/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp24/lib/python3.12/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mto_tensor(pic)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp24/lib/python3.12/site-packages/torchvision/transforms/functional.py:176\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    174\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdefault_float_dtype)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(classifier.linear_proj.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "# Training loop for transfer learning\n",
    "num_epochs = 15  # Adjust the number of epochs as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    classifier.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in tqdm(train_classification_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = classifier(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_accuracy = 100 * correct / total\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_classification_loader):.4f}, '\n",
    "          f'Train Accuracy: {train_accuracy:.2f}%')\n",
    "\n",
    "    # Validation loop\n",
    "    classifier.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_classification_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = classifier(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * correct / total\n",
    "    avg_val_loss = val_loss / len(val_classification_loader)\n",
    "    print(f'Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bda62d0-45c8-4d97-a868-02608256f63a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now running only linear unfrozen and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e316dd18-f7e0-49a0-a392-d7060ff8c9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "15 epoch w/ supposedly pretrained weights and linear cls head\n",
    "\n",
    "100%|██████████| 1563/1563 [02:37<00:00,  9.89it/s]\n",
    "Epoch [1/15], Loss: 5.3484, Train Accuracy: 1.14%\n",
    "Validation Loss: 5.2455, Validation Accuracy: 2.05%\n",
    "100%|██████████| 1563/1563 [02:28<00:00, 10.50it/s]\n",
    "Epoch [2/15], Loss: 5.2022, Train Accuracy: 2.18%\n",
    "Validation Loss: 5.1732, Validation Accuracy: 2.21%\n",
    "100%|██████████| 1563/1563 [02:24<00:00, 10.79it/s]\n",
    "Epoch [3/15], Loss: 5.1099, Train Accuracy: 2.87%\n",
    "Validation Loss: 5.0519, Validation Accuracy: 2.84%\n",
    "100%|██████████| 1563/1563 [02:26<00:00, 10.68it/s]\n",
    "Epoch [4/15], Loss: 5.0460, Train Accuracy: 3.43%\n",
    "Validation Loss: 5.0245, Validation Accuracy: 3.62%\n",
    "100%|██████████| 1563/1563 [02:24<00:00, 10.85it/s]\n",
    "Epoch [5/15], Loss: 4.9947, Train Accuracy: 3.78%\n",
    "Validation Loss: 4.9768, Validation Accuracy: 4.19%\n",
    "100%|██████████| 1563/1563 [02:25<00:00, 10.75it/s]\n",
    "Epoch [6/15], Loss: 4.9522, Train Accuracy: 4.26%\n",
    "Validation Loss: 4.9880, Validation Accuracy: 3.43%\n",
    "100%|██████████| 1563/1563 [03:02<00:00,  8.58it/s]\n",
    "Epoch [7/15], Loss: 4.9156, Train Accuracy: 4.53%\n",
    "Validation Loss: 4.8988, Validation Accuracy: 4.77%\n",
    "100%|██████████| 1563/1563 [02:24<00:00, 10.82it/s]\n",
    "Epoch [8/15], Loss: 4.8836, Train Accuracy: 4.96%\n",
    "Validation Loss: 4.8678, Validation Accuracy: 5.11%\n",
    "100%|██████████| 1563/1563 [02:34<00:00, 10.12it/s]\n",
    "Epoch [9/15], Loss: 4.8599, Train Accuracy: 5.09%\n",
    "Validation Loss: 4.9123, Validation Accuracy: 4.35%\n",
    "100%|██████████| 1563/1563 [02:26<00:00, 10.68it/s]\n",
    "Epoch [10/15], Loss: 4.8314, Train Accuracy: 5.39%\n",
    "Validation Loss: 4.8441, Validation Accuracy: 5.19%\n",
    "100%|██████████| 1563/1563 [02:26<00:00, 10.69it/s]\n",
    "Epoch [11/15], Loss: 4.8131, Train Accuracy: 5.66%\n",
    "Validation Loss: 4.8576, Validation Accuracy: 5.14%\n",
    "100%|██████████| 1563/1563 [02:28<00:00, 10.50it/s]\n",
    "Epoch [12/15], Loss: 4.7973, Train Accuracy: 5.63%\n",
    "Validation Loss: 4.8183, Validation Accuracy: 5.17%\n",
    "100%|██████████| 1563/1563 [02:25<00:00, 10.75it/s]\n",
    "Epoch [13/15], Loss: 4.7789, Train Accuracy: 6.00%\n",
    "Validation Loss: 4.8186, Validation Accuracy: 5.46%\n",
    "100%|██████████| 1563/1563 [02:27<00:00, 10.58it/s]\n",
    "Epoch [14/15], Loss: 4.7599, Train Accuracy: 6.20%\n",
    "Validation Loss: 4.7920, Validation Accuracy: 5.58%\n",
    "100%|██████████| 1563/1563 [02:24<00:00, 10.81it/s]\n",
    "Epoch [15/15], Loss: 4.7487, Train Accuracy: 6.28%\n",
    "Validation Loss: 4.7666, Validation Accuracy: 5.74%\n",
    "'''\n",
    "\n",
    "'''\n",
    "15 epoch with supposedly Randomly initialized  weights :( better performance. Why ?\n",
    "100%|██████████| 1563/1563 [02:27<00:00, 10.63it/s]\n",
    "Epoch [1/15], Loss: 5.1392, Train Accuracy: 2.68%\n",
    "Validation Loss: 4.9973, Validation Accuracy: 3.80%\n",
    "100%|██████████| 1563/1563 [02:37<00:00,  9.95it/s]\n",
    "Epoch [2/15], Loss: 4.9252, Train Accuracy: 4.47%\n",
    "Validation Loss: 4.8776, Validation Accuracy: 4.80%\n",
    "100%|██████████| 1563/1563 [02:45<00:00,  9.47it/s]\n",
    "Epoch [3/15], Loss: 4.8237, Train Accuracy: 5.38%\n",
    "Validation Loss: 4.8147, Validation Accuracy: 5.20%\n",
    "100%|██████████| 1563/1563 [02:44<00:00,  9.51it/s]\n",
    "Epoch [4/15], Loss: 4.7573, Train Accuracy: 6.05%\n",
    "Validation Loss: 4.7609, Validation Accuracy: 6.31%\n",
    "100%|██████████| 1563/1563 [02:30<00:00, 10.38it/s]\n",
    "Epoch [5/15], Loss: 4.7052, Train Accuracy: 6.87%\n",
    "Validation Loss: 4.7215, Validation Accuracy: 6.67%\n",
    "100%|██████████| 1563/1563 [02:49<00:00,  9.22it/s]\n",
    "Epoch [6/15], Loss: 4.6665, Train Accuracy: 7.15%\n",
    "Validation Loss: 4.6870, Validation Accuracy: 6.96%\n",
    "100%|██████████| 1563/1563 [02:34<00:00, 10.08it/s]\n",
    "Epoch [7/15], Loss: 4.6338, Train Accuracy: 7.60%\n",
    "Validation Loss: 4.6672, Validation Accuracy: 7.28%\n",
    "100%|██████████| 1563/1563 [02:55<00:00,  8.89it/s]\n",
    "Epoch [8/15], Loss: 4.6013, Train Accuracy: 7.87%\n",
    "Validation Loss: 4.6382, Validation Accuracy: 7.85%\n",
    "100%|██████████| 1563/1563 [02:26<00:00, 10.63it/s]\n",
    "Epoch [9/15], Loss: 4.5801, Train Accuracy: 8.12%\n",
    "Validation Loss: 4.6449, Validation Accuracy: 7.59%\n",
    "100%|██████████| 1563/1563 [02:49<00:00,  9.22it/s]\n",
    "Epoch [10/15], Loss: 4.5573, Train Accuracy: 8.68%\n",
    "Validation Loss: 4.6266, Validation Accuracy: 7.88%\n",
    "100%|██████████| 1563/1563 [02:27<00:00, 10.61it/s]\n",
    "Epoch [11/15], Loss: 4.5364, Train Accuracy: 8.79%\n",
    "Validation Loss: 4.6232, Validation Accuracy: 7.71%\n",
    "100%|██████████| 1563/1563 [02:33<00:00, 10.16it/s]\n",
    "Epoch [12/15], Loss: 4.5180, Train Accuracy: 9.10%\n",
    "Validation Loss: 4.6077, Validation Accuracy: 7.74%\n",
    "100%|██████████| 1563/1563 [02:26<00:00, 10.68it/s]\n",
    "Epoch [13/15], Loss: 4.5026, Train Accuracy: 9.20%\n",
    "Validation Loss: 4.5907, Validation Accuracy: 8.20%\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f37120-cf07-4547-8f19-4a2ec69c2621",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
