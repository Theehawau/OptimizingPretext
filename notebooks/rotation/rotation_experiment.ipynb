{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NwWFnHpbBTOe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QG87kI_DO_f",
        "outputId": "33d9d759-ced1-4e99-be73-c2d3e92e7e41"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility -> maybe use pytorch lightning for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjtznEIIDG3J",
        "outputId": "e7a71e8c-9cd2-4c6c-9cdc-995622bcd910"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x796de6f0d0d0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tinyImageNet_dataset = load_dataset(\"zh-plus/tiny-imagenet\")"
      ],
      "metadata": {
        "id": "h9wnxddlDeJ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f94c5b3-22cc-4058-c6b4-5ea4f6cbbffb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tinyImageNet_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qs92wOfNDm3L",
        "outputId": "5351a5aa-cef2-476d-a73d-392b31f344d2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['image', 'label'],\n",
              "        num_rows: 100000\n",
              "    })\n",
              "    valid: Dataset({\n",
              "        features: ['image', 'label'],\n",
              "        num_rows: 10000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rotate_img(img, rot):\n",
        "    if rot == 0:  # 0 degrees rotation\n",
        "        return img\n",
        "    elif rot == 90:  # 90 degrees rotation\n",
        "        return np.flipud(np.transpose(img, (1, 0, 2)))\n",
        "    elif rot == 180:  # 180 degrees rotation\n",
        "        return np.fliplr(np.flipud(img))\n",
        "    elif rot == 270:  # 270 degrees rotation\n",
        "        return np.transpose(np.flipud(img), (1, 0, 2))\n",
        "    else:\n",
        "        raise ValueError('Rotation should be 0, 90, 180, or 270 degrees.')\n",
        "\n",
        "class RotationDataset(data.Dataset):\n",
        "    def __init__(self, hf_dataset, transform=None):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            hf_dataset: HuggingFace Dataset object.\n",
        "            transform: Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.dataset = hf_dataset\n",
        "        self.transform = transform\n",
        "        self.rotations = [0, 90, 180, 270]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image from the HuggingFace dataset and convert to RGB\n",
        "        image = self.dataset[idx]['image'].convert('RGB')  # Ensure image is in RGB\n",
        "\n",
        "        # We'll resize it to 255x255 since this is ResNet's input size\n",
        "        image = image.resize((255, 255))\n",
        "\n",
        "        # if self.transform:\n",
        "        #     image = self.transform(image)\n",
        "\n",
        "        # Create four rotated versions of the image and corresponding labels (0, 1, 2, 3 for 0°, 90°, 180°, 270°)\n",
        "        rotated_imgs = []\n",
        "        for rot in self.rotations:\n",
        "            rotated_image = rotate_img(np.array(image), rot)  # Apply rotation\n",
        "            rotated_image = Image.fromarray(rotated_image)    # Convert back to PIL Image\n",
        "            rotated_image = self.transform(rotated_image)     # Apply transformations\n",
        "            rotated_imgs.append(rotated_image)\n",
        "        rotation_labels = torch.LongTensor([0, 1, 2, 3])\n",
        "\n",
        "        # Stack the rotated images into a tensor\n",
        "        rotated_imgs_tensor = torch.stack(rotated_imgs, dim=0)  # Shape: [4, 3, H, W] for 4 rotations\n",
        "\n",
        "        return rotated_imgs_tensor, rotation_labels\n"
      ],
      "metadata": {
        "id": "LwfFZLkSIgIu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),transforms.ToTensor()])"
      ],
      "metadata": {
        "id": "1DPYG-ypTYEb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RotationNet(nn.Module):\n",
        "    def __init__(self,\n",
        "                 n_rotations=4,  # 4 rotations: 0°, 90°, 180°, 270°\n",
        "                 architecture = 'resnet', # 'resnet' or 'vit'\n",
        "                ):\n",
        "\n",
        "        super(RotationNet, self).__init__()\n",
        "\n",
        "        if architecture=='resnet':\n",
        "            # Backbone ResNet model TODO: replace by ResNet 50\n",
        "            # self.resnet = models.resnet18(pretrained=False) # I thnk this is deprecated\n",
        "            self.resnet = models.resnet18()\n",
        "            self.resnet.fc = nn.Identity()  #Remove the classification layer\n",
        "\n",
        "        elif architecture=='vit':\n",
        "            pass ##TODO\n",
        "\n",
        "        # Fully connected layers << to dispose after the PTT\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(512, 128),  # TODO not sure abt dims?\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, n_rotations)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [batch_size, 3, 64, 64]\n",
        "        features = self.resnet(x)  # Shape: [batch_size, 512])\n",
        "\n",
        "        #\n",
        "        out = self.fc(features)  # Shape: [batch_size, n_rotations]\n",
        "        return out"
      ],
      "metadata": {
        "id": "Aitu-R9MDq6z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RotationNet(n_rotations=4)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "id": "VziyMxogHwtK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create the datasets and dataloaders\n",
        "train_dataset = RotationDataset(tinyImageNet_dataset['train'], transform=transform)\n",
        "valid_dataset = RotationDataset(tinyImageNet_dataset['valid'], transform=transform)\n",
        "\n",
        "train_loader = data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
        "valid_loader = data.DataLoader(valid_dataset, batch_size=128, shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "id": "8wXZLpdjaGuk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3a87baa-7e4d-4250-ccde-c4e0e8360bff"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WZ-yJ5age7q5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Shape of dataset output: {}'.format(next(iter(train_loader))[0].shape))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61nqkQrIaQ1F",
        "outputId": "0dbc6f5d-c46a-4633-91df-2eca5be588ad"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of dataset output: torch.Size([128, 4, 3, 255, 255])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n",
        "num_epochs = 10\n",
        "log_each = 100"
      ],
      "metadata": {
        "id": "tdz4FUQTaqeu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    avg_loss = 0.0\n",
        "    for batch_idx, (rotated_imgs, rotation_labels) in enumerate(tqdm(train_loader)):\n",
        "        rotated_imgs = rotated_imgs.view(-1, 3, 255, 255).to(device)  # Shape: [batch_size * 4, 3, 255, 255]\n",
        "        rotation_labels = rotation_labels.view(-1).to(device)  # Shape: [batch_size * 4]\n",
        "\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(rotated_imgs)  # Shape: [batch_size * 4, n_rotations]\n",
        "        loss = criterion(outputs, rotation_labels)  # Shape: [batch_size * 4]\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        avg_loss += loss.item()\n",
        "        if batch_idx % log_each == log_each - 1:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}], Loss: {avg_loss / 100:.4f}')\n",
        "            avg_loss = 0.0\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for rotated_imgs, rotation_labels in tqdm(valid_loader):\n",
        "            rotated_imgs = rotated_imgs.view(-1, 3, 255, 255).to(device)  # Shape: [batch_size * 4, 3, 255, 255]\n",
        "            rotation_labels = rotation_labels.view(-1).to(device)  # Shape: [batch_size * 4]\n",
        "\n",
        "            outputs = model(rotated_imgs)  # Shape: [batch_size * 4, n_rotations]\n",
        "            loss = criterion(outputs, rotation_labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += rotation_labels.size(0)\n",
        "            correct += (predicted == rotation_labels).sum().item()\n",
        "\n",
        "    val_accuracy = 100 * correct / total\n",
        "    avg_val_loss = val_loss / len(valid_loader)\n",
        "    print(f'Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meQz5Iq4fAIh",
        "outputId": "a6e4be11-5f6b-4db8-c846-230217fc34d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/782 [00:00<?, ?it/s]"
          ]
        }
      ]
    }
  ]
}